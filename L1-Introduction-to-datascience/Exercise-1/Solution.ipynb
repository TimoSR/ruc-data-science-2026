{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b9aa53",
   "metadata": {},
   "source": [
    "**EXERCISE:** Answer the following questions based on the Adult dataset\n",
    "1. What is the mean age of all persons in the data?\n",
    "1. What is the mean age of female persons? What about male persons?\n",
    "1. How many different types of educations are there?\n",
    "2. What are the different types of education and how many persons are the for each type?\n",
    "3. Is there a difference in educational level across sex?\n",
    "4. What is the most common relationship status?\n",
    "5. Is there a correlation between hours per week (worked) and age?\n",
    "6. Is the average hours per week (worked) different across different marital-status groups?\n",
    "7. Is there an income difference across sexes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e77ab340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e429f429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "pandas.read_csv(\n",
      "    filepath_or_buffer: \u001b[33m'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]'\u001b[39m,\n",
      "    *,\n",
      "    sep: \u001b[33m'str | None | lib.NoDefault'\u001b[39m = <no_default>,\n",
      "    delimiter: \u001b[33m'str | None | lib.NoDefault'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    header: \u001b[33m\"int | Sequence[int] | None | Literal['infer']\"\u001b[39m = \u001b[33m'infer'\u001b[39m,\n",
      "    names: \u001b[33m'Sequence[Hashable] | None | lib.NoDefault'\u001b[39m = <no_default>,\n",
      "    index_col: \u001b[33m'IndexLabel | Literal[False] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    usecols: \u001b[33m'UsecolsArgType'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    dtype: \u001b[33m'DtypeArg | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    engine: \u001b[33m'CSVEngine | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    converters: \u001b[33m'Mapping[HashableT, Callable] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    true_values: \u001b[33m'list | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    false_values: \u001b[33m'list | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    skipinitialspace: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    skiprows: \u001b[33m'list[int] | int | Callable[[Hashable], bool] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    skipfooter: \u001b[33m'int'\u001b[39m = \u001b[32m0\u001b[39m,\n",
      "    nrows: \u001b[33m'int | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    na_values: \u001b[33m'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    keep_default_na: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    na_filter: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    skip_blank_lines: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    parse_dates: \u001b[33m'bool | Sequence[Hashable] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    date_format: \u001b[33m'str | dict[Hashable, str] | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    dayfirst: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    cache_dates: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    iterator: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    chunksize: \u001b[33m'int | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    compression: \u001b[33m'CompressionOptions'\u001b[39m = \u001b[33m'infer'\u001b[39m,\n",
      "    thousands: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    decimal: \u001b[33m'str'\u001b[39m = \u001b[33m'.'\u001b[39m,\n",
      "    lineterminator: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    quotechar: \u001b[33m'str'\u001b[39m = \u001b[33m'\"'\u001b[39m,\n",
      "    quoting: \u001b[33m'int'\u001b[39m = \u001b[32m0\u001b[39m,\n",
      "    doublequote: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    escapechar: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    comment: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    encoding: \u001b[33m'str | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    encoding_errors: \u001b[33m'str | None'\u001b[39m = \u001b[33m'strict'\u001b[39m,\n",
      "    dialect: \u001b[33m'str | csv.Dialect | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    on_bad_lines: \u001b[33m'str'\u001b[39m = \u001b[33m'error'\u001b[39m,\n",
      "    low_memory: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    memory_map: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    float_precision: \u001b[33m\"Literal['high', 'legacy', 'round_trip'] | None\"\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    storage_options: \u001b[33m'StorageOptions | None'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    dtype_backend: \u001b[33m'DtypeBackend | lib.NoDefault'\u001b[39m = <no_default>,\n",
      ") -> \u001b[33m'DataFrame | TextFileReader'\u001b[39m\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Read a comma-separated values (csv) file into DataFrame.\n",
      "\n",
      "Also supports optionally iterating or breaking of the file\n",
      "into chunks.\n",
      "\n",
      "Additional help can be found in the online docs for\n",
      "`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "filepath_or_buffer : str, path object or file-like object\n",
      "    Any valid string path is acceptable. The string could be a URL. Valid\n",
      "    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "    expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "\n",
      "    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "\n",
      "    By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "sep : str, default ','\n",
      "    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
      "    C engine cannot automatically detect\n",
      "    the separator, but the Python parsing engine can, meaning the latter will\n",
      "    be used and automatically detect the separator from only the first valid\n",
      "    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n",
      "    In addition, separators longer than 1 character and different from\n",
      "    ``'\\s+'`` will be interpreted as regular expressions and will also force\n",
      "    the use of the Python parsing engine. Note that regex delimiters are prone\n",
      "    to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "delimiter : str, optional\n",
      "    Alias for ``sep``.\n",
      "header : int, Sequence of int, 'infer' or None, default 'infer'\n",
      "    Row number(s) containing column labels and marking the start of the\n",
      "    data (zero-indexed). Default behavior is to infer the column names:\n",
      "    if no ``names``\n",
      "    are passed the behavior is identical to ``header=0`` and column\n",
      "    names are inferred from the first line of the file, if column\n",
      "    names are passed explicitly to ``names`` then the behavior is identical to\n",
      "    ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "    replace existing names. The header can be a list of integers that\n",
      "    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
      "    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
      "    skipped (e.g. 2 in this example is skipped). Note that this\n",
      "    parameter ignores commented lines and empty lines if\n",
      "    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "    data rather than the first line of the file.\n",
      "\n",
      "    When inferred from the file contents, headers are kept distinct from\n",
      "    each other by renaming duplicate names with a numeric suffix of the form\n",
      "    ``\".{{count}}\"`` starting from 1, e.g. ``\"foo\"`` and ``\"foo.1\"``.\n",
      "    Empty headers are named ``\"Unnamed: {{i}}\"`` or ``\n",
      "    \"Unnamed: {{i}}_level_{{level}}\"``\n",
      "    in the case of MultiIndex columns.\n",
      "names : Sequence of Hashable, optional\n",
      "    Sequence of column labels to apply. If the file contains a header row,\n",
      "    then you should explicitly pass ``header=0`` to override the column names.\n",
      "    Duplicates in this list are not allowed.\n",
      "index_col : Hashable, Sequence of Hashable or False, optional\n",
      "    Column(s) to use as row label(s), denoted either by column labels or column\n",
      "    indices.  If a sequence of labels or indices is given,\n",
      "    :class:`~pandas.MultiIndex`\n",
      "    will be formed for the row labels.\n",
      "\n",
      "    Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "    column as the index, e.g., when you have a malformed file with delimiters at\n",
      "    the end of each line.\n",
      "usecols : Sequence of Hashable or Callable, optional\n",
      "    Subset of columns to select, denoted either\n",
      "    by column labels or column indices.\n",
      "    If list-like, all elements must either\n",
      "    be positional (i.e. integer indices into the document columns) or strings\n",
      "    that correspond to column names provided either by the user in ``names`` or\n",
      "    inferred from the document header row(s).\n",
      "    If ``names`` are given, the document\n",
      "    header row(s) are not taken into account. For example, a valid list-like\n",
      "    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
      "    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n",
      "    for columns in ``['foo', 'bar']`` order or\n",
      "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "    for ``['bar', 'foo']`` order.\n",
      "\n",
      "    If callable, the callable function will be evaluated against the column\n",
      "    names, returning names where the callable function evaluates to ``True``. An\n",
      "    example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "    parsing time and lower memory usage.\n",
      "dtype : dtype or dict of {{Hashable : dtype}}, optional\n",
      "    Data type(s) to apply to either the whole dataset or individual columns.\n",
      "    E.g., ``{{'a': np.float64, 'b': np.int32, 'c': 'Int64'}}``\n",
      "    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
      "    to preserve and not interpret ``dtype``.\n",
      "    If ``converters`` are specified, they will be applied INSTEAD\n",
      "    of ``dtype`` conversion. Specify a ``defaultdict`` as input where\n",
      "    the default determines the ``dtype``\n",
      "    of the columns which are not explicitly\n",
      "    listed.\n",
      "engine : {{'c', 'python', 'pyarrow'}}, optional\n",
      "    Parser engine to use. The C and pyarrow engines are faster,\n",
      "    while the python engine\n",
      "    is currently more feature-complete. Multithreading\n",
      "    is currently only supported by\n",
      "    the pyarrow engine. Some features of the \"pyarrow\" engine\n",
      "    are unsupported or may not work correctly.\n",
      "converters : dict of {{Hashable : Callable}}, optional\n",
      "    Functions for converting values in specified columns. Keys can either\n",
      "    be column labels or column indices.\n",
      "true_values : list, optional\n",
      "    Values to consider as ``True`` in addition\n",
      "    to case-insensitive variants of 'True'.\n",
      "false_values : list, optional\n",
      "    Values to consider as ``False`` in addition to case-insensitive\n",
      "    variants of 'False'.\n",
      "skipinitialspace : bool, default False\n",
      "    Skip spaces after delimiter.\n",
      "skiprows : int, list of int or Callable, optional\n",
      "    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
      "    at the start of the file.\n",
      "\n",
      "    If callable, the callable function will be evaluated against the row\n",
      "    indices, returning ``True`` if the row should be skipped and ``False``\n",
      "    otherwise.\n",
      "    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "skipfooter : int, default 0\n",
      "    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\n",
      "nrows : int, optional\n",
      "    Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    Refers to the number of data rows in the returned DataFrame, excluding:\n",
      "\n",
      "    * The header row containing column names.\n",
      "    * Rows before the header row, if ``header=1`` or larger.\n",
      "\n",
      "    Example usage:\n",
      "\n",
      "    * To read the first 999,999 (non-header) rows:\n",
      "      ``read_csv(..., nrows=999999)``\n",
      "\n",
      "    * To read rows 1,000,000 through 1,999,999:\n",
      "      ``read_csv(..., skiprows=1000000, nrows=999999)``\n",
      "\n",
      "na_values : Hashable, Iterable of Hashable or dict of {{Hashable : Iterable}},\n",
      "    optional\n",
      "    Additional strings to recognize as ``NA``/``NaN``. If ``dict``\n",
      "    passed, specific\n",
      "    per-column ``NA`` values.  By default the following values\n",
      "    are interpreted as\n",
      "    ``NaN``: empty string, \"NaN\", \"N/A\", \"NULL\", and other common\n",
      "    representations of missing data.\n",
      "keep_default_na : bool, default True\n",
      "    Whether or not to include the default ``NaN`` values when parsing the data.\n",
      "    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
      "\n",
      "    * If ``keep_default_na`` is ``True``, and ``na_values``\n",
      "      are specified, ``na_values``\n",
      "      is appended to the default ``NaN`` values used for parsing.\n",
      "    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
      "      the default ``NaN`` values are used for parsing.\n",
      "    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
      "      the ``NaN`` values specified ``na_values`` are used for parsing.\n",
      "    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
      "      strings will be parsed as ``NaN``.\n",
      "\n",
      "    Note that if ``na_filter`` is passed in as ``False``,\n",
      "    the ``keep_default_na`` and\n",
      "    ``na_values`` parameters will be ignored.\n",
      "na_filter : bool, default True\n",
      "    Detect missing value markers (empty strings and the value of ``na_values``). In\n",
      "    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
      "    performance of reading a large file.\n",
      "skip_blank_lines : bool, default True\n",
      "    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
      "parse_dates : bool, None, list of Hashable, default None\n",
      "    The behavior is as follows:\n",
      "\n",
      "    * ``bool``. If ``True`` -> try parsing the index.\n",
      "    * ``None``. Behaves like ``True`` if ``date_format`` is specified.\n",
      "    * ``list`` of ``int`` or names.\n",
      "      e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n",
      "      each as a separate date column.\n",
      "\n",
      "    If a column or index cannot be represented as an array of ``datetime``,\n",
      "    say because of an unparsable value or a mixture of timezones, the column\n",
      "    or index will be returned unaltered as an ``object`` data type. For\n",
      "    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
      "    :func:`~pandas.read_csv`.\n",
      "\n",
      "    Note: A fast-path exists for iso8601-formatted dates.\n",
      "date_format : str or dict of column -> format, optional\n",
      "    Format to use for parsing dates and/or times when\n",
      "    used in conjunction with ``parse_dates``.\n",
      "    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
      "    `strftime documentation\n",
      "    <https://docs.python.org/3/library/datetime.html\n",
      "    #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
      "    note that :const:`\"%f\"`` will parse all the way up to nanoseconds.\n",
      "    You can also pass:\n",
      "\n",
      "    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
      "      time string (not necessarily in exactly the same format);\n",
      "    - \"mixed\", to infer the format for each element individually. This is risky,\n",
      "      and you should probably use it along with `dayfirst`.\n",
      "\n",
      "    .. versionadded:: 2.0.0\n",
      "dayfirst : bool, default False\n",
      "    DD/MM format dates, international and European format.\n",
      "cache_dates : bool, default True\n",
      "    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
      "    conversion. May produce significant speed-up when parsing duplicate\n",
      "    date strings, especially ones with timezone offsets.\n",
      "\n",
      "iterator : bool, default False\n",
      "    Return ``TextFileReader`` object for iteration or getting chunks with\n",
      "    ``get_chunk()``.\n",
      "chunksize : int, optional\n",
      "    Number of lines to read from the file per chunk. Passing a value will cause the\n",
      "    function to return a ``TextFileReader`` object for iteration.\n",
      "    See the `IO Tools docs\n",
      "    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "    for more information on ``iterator`` and ``chunksize``.\n",
      "\n",
      "compression : str or dict, default 'infer'\n",
      "    For on-the-fly decompression of on-disk data.\n",
      "    If 'infer' and 'filepath_or_buffer' is\n",
      "    path-like, then detect compression from the following extensions: '.gz',\n",
      "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "    (otherwise no compression).\n",
      "    If using 'zip' or 'tar', the ZIP file must contain only\n",
      "    one data file to be read in.\n",
      "    Set to ``None`` for no decompression.\n",
      "    Can also be a dict with key ``'method'`` set\n",
      "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``,\n",
      "    ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "    other key-value pairs are forwarded to\n",
      "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
      "    ``tarfile.TarFile``, respectively.\n",
      "    As an example, the following could be passed for\n",
      "    Zstandard decompression using a\n",
      "    custom compression dictionary:\n",
      "    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "\n",
      "thousands : str (length 1), optional\n",
      "    Character acting as the thousands separator in numerical values.\n",
      "decimal : str (length 1), default '.'\n",
      "    Character to recognize as decimal point (e.g., use ',' for European data).\n",
      "lineterminator : str (length 1), optional\n",
      "    Character used to denote a line break. Only valid with C parser.\n",
      "quotechar : str (length 1), optional\n",
      "    Character used to denote the start and end of a quoted item. Quoted\n",
      "    items can include the ``delimiter`` and it will be ignored.\n",
      "quoting : {{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL,\n",
      "    2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}}, default csv.QUOTE_MINIMAL\n",
      "    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
      "    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that\n",
      "    only fields containing special\n",
      "    characters are quoted (e.g., characters defined\n",
      "    in ``quotechar``, ``delimiter``,\n",
      "    or ``lineterminator``.\n",
      "doublequote : bool, default True\n",
      "    When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
      "    whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
      "    field as a single ``quotechar`` element.\n",
      "escapechar : str (length 1), optional\n",
      "    Character used to escape other characters.\n",
      "comment : str (length 1), optional\n",
      "    Character indicating that the remainder of line should not be parsed.\n",
      "    If found at the beginning\n",
      "    of a line, the line will be ignored altogether. This parameter must be a\n",
      "    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "    fully commented lines are ignored by the parameter ``header`` but not by\n",
      "    ``skiprows``. For example, if ``comment='#'``, parsing\n",
      "    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n",
      "    treated as the header.\n",
      "encoding : str, optional, default 'utf-8'\n",
      "    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n",
      "    standard encodings\n",
      "    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "\n",
      "encoding_errors : str, optional, default 'strict'\n",
      "    How encoding errors are treated. `List of possible values\n",
      "    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "\n",
      "dialect : str or csv.Dialect, optional\n",
      "    If provided, this parameter will override values (default or not) for the\n",
      "    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
      "    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
      "    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
      "    documentation for more details.\n",
      "on_bad_lines : {{'error', 'warn', 'skip'}} or Callable, default 'error'\n",
      "    Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "    Allowed values are:\n",
      "\n",
      "    - ``'error'``, raise an Exception when a bad line is encountered.\n",
      "    - ``'warn'``, raise a warning when a bad line is\n",
      "      encountered and skip that line.\n",
      "    - ``'skip'``, skip bad lines without raising or warning when\n",
      "      they are encountered.\n",
      "    - Callable, function that will process a single bad line.\n",
      "        - With ``engine='python'``, function with signature\n",
      "          ``(bad_line: list[str]) -> list[str] | None``.\n",
      "          ``bad_line`` is a list of strings split by the ``sep``.\n",
      "          If the function returns ``None``, the bad line will be ignored.\n",
      "          If the function returns a new ``list`` of strings with\n",
      "          more elements than\n",
      "          expected, a ``ParserWarning`` will be emitted while\n",
      "          dropping extra elements.\n",
      "        - With ``engine='pyarrow'``, function with signature\n",
      "          as described in pyarrow documentation: `invalid_row_handler\n",
      "          <https://arrow.apache.org/docs/python\n",
      "          /generated/pyarrow.csv.ParseOptions.html\n",
      "          #pyarrow.csv.ParseOptions.invalid_row_handler>`_.\n",
      "\n",
      "    .. versionchanged:: 2.2.0\n",
      "\n",
      "        Callable for ``engine='pyarrow'``\n",
      "\n",
      "low_memory : bool, default True\n",
      "    Internally process the file in chunks, resulting in lower memory use\n",
      "    while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "    types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
      "    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
      "    regardless, use the ``chunksize`` or ``iterator``\n",
      "    parameter to return the data in\n",
      "    chunks. (Only valid with C parser).\n",
      "memory_map : bool, default False\n",
      "    If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
      "    directly onto memory and access the data directly from there. Using this\n",
      "    option can improve performance because there is no longer any I/O overhead.\n",
      "float_precision : {{'high', 'legacy', 'round_trip'}}, optional\n",
      "    Specifies which converter the C engine should use for floating-point\n",
      "    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n",
      "    ``'legacy'`` for the original lower precision pandas converter, and\n",
      "    ``'round_trip'`` for the round-trip converter.\n",
      "\n",
      "storage_options : dict, optional\n",
      "    Extra options that make sense for a particular storage connection, e.g.\n",
      "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "    details, and for more examples on storage options refer `here\n",
      "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "    highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "dtype_backend : {{'numpy_nullable', 'pyarrow'}}\n",
      "    Back-end data type applied to the resultant :class:`DataFrame`\n",
      "    (still experimental). If not specified, the default behavior\n",
      "    is to not use nullable data types. If specified, the behavior\n",
      "    is as follows:\n",
      "\n",
      "    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
      "    * ``\"pyarrow\"``: returns\n",
      "      pyarrow-backed nullable :class:`ArrowDtype` :class:`DataFrame`\n",
      "\n",
      "    .. versionadded:: 2.0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DataFrame or TextFileReader\n",
      "    A comma-separated values (csv) file is returned as two-dimensional\n",
      "    data structure with labeled axes.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "read_table : Read general delimited file into DataFrame.\n",
      "read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> pd.read_csv(\"data.csv\")  # doctest: +SKIP\n",
      "   Name  Value\n",
      "0   foo      1\n",
      "1   bar      2\n",
      "2  #baz      3\n",
      "\n",
      "Index and header can be specified via the `index_col` and `header` arguments.\n",
      "\n",
      ">>> pd.read_csv(\"data.csv\", header=None)  # doctest: +SKIP\n",
      "      0      1\n",
      "0  Name  Value\n",
      "1   foo      1\n",
      "2   bar      2\n",
      "3  #baz      3\n",
      "\n",
      ">>> pd.read_csv(\"data.csv\", index_col=\"Value\")  # doctest: +SKIP\n",
      "       Name\n",
      "Value\n",
      "1       foo\n",
      "2       bar\n",
      "3      #baz\n",
      "\n",
      "Column types are inferred but can be explicitly specified using the dtype argument.\n",
      "\n",
      ">>> pd.read_csv(\"data.csv\", dtype={{\"Value\": float}})  # doctest: +SKIP\n",
      "   Name  Value\n",
      "0   foo    1.0\n",
      "1   bar    2.0\n",
      "2  #baz    3.0\n",
      "\n",
      "True, False, and NA values, and thousands separators have defaults,\n",
      "but can be explicitly specified, too. Supply the values you would like\n",
      "as strings or lists of strings!\n",
      "\n",
      ">>> pd.read_csv(\"data.csv\", na_values=[\"foo\", \"bar\"])  # doctest: +SKIP\n",
      "   Name  Value\n",
      "0   NaN      1\n",
      "1   NaN      2\n",
      "2  #baz      3\n",
      "\n",
      "Comment lines in the input file can be skipped using the `comment` argument.\n",
      "\n",
      ">>> pd.read_csv(\"data.csv\", comment=\"#\")  # doctest: +SKIP\n",
      "  Name  Value\n",
      "0  foo      1\n",
      "1  bar      2\n",
      "\n",
      "By default, columns with dates will be read as ``object`` rather than  ``datetime``.\n",
      "\n",
      ">>> df = pd.read_csv(\"tmp.csv\")  # doctest: +SKIP\n",
      "\n",
      ">>> df  # doctest: +SKIP\n",
      "   col 1       col 2            col 3\n",
      "0     10  10/04/2018  Sun 15 Jan 2023\n",
      "1     20  15/04/2018  Fri 12 May 2023\n",
      "\n",
      ">>> df.dtypes  # doctest: +SKIP\n",
      "col 1     int64\n",
      "col 2    object\n",
      "col 3    object\n",
      "dtype: object\n",
      "\n",
      "Specific columns can be parsed as dates by using the `parse_dates` and\n",
      "`date_format` arguments.\n",
      "\n",
      ">>> df = pd.read_csv(\n",
      "...     \"tmp.csv\",\n",
      "...     parse_dates=[1, 2],\n",
      "...     date_format={{\"col 2\": \"%d/%m/%Y\", \"col 3\": \"%a %d %b %Y\"}},\n",
      "... )  # doctest: +SKIP\n",
      "\n",
      ">>> df.dtypes  # doctest: +SKIP\n",
      "col 1             int64\n",
      "col 2    datetime64[ns]\n",
      "col 3    datetime64[ns]\n",
      "dtype: object\n",
      "\u001b[31mFile:\u001b[39m      c:\\users\\timot\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "?pandas.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5644127",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pandas.read_csv(\"adult.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39a43aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age         workclass  fnlwgt  education  education-num  \\\n",
      "0   39         State-gov   77516  Bachelors             13   \n",
      "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
      "2   38           Private  215646    HS-grad              9   \n",
      "3   53           Private  234721       11th              7   \n",
      "4   28           Private  338409  Bachelors             13   \n",
      "\n",
      "       marital-status         occupation   relationship   race     sex  \\\n",
      "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week native-country income  \n",
      "0          2174             0              40  United-States  <=50K  \n",
      "1             0             0              13  United-States  <=50K  \n",
      "2             0             0              40  United-States  <=50K  \n",
      "3             0             0              40  United-States  <=50K  \n",
      "4             0             0              40           Cuba  <=50K  \n"
     ]
    }
   ],
   "source": [
    "print(data_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ee80d7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  48842.000000  4.884200e+04   48842.000000  48842.000000  48842.000000   \n",
      "mean      38.643585  1.896641e+05      10.078089   1079.067626     87.502314   \n",
      "std       13.710510  1.056040e+05       2.570973   7452.019058    403.004552   \n",
      "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.175505e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.781445e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.376420e+05      12.000000      0.000000      0.000000   \n",
      "max       90.000000  1.490400e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours-per-week  \n",
      "count    48842.000000  \n",
      "mean        40.422382  \n",
      "std         12.391444  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n"
     ]
    }
   ],
   "source": [
    "print(data_frame.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4aa2a",
   "metadata": {},
   "source": [
    "### What is the mean age of all persons in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f55944f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.64358543876172\n"
     ]
    }
   ],
   "source": [
    "mean_age = data_frame[\"age\"].mean()\n",
    "\n",
    "print(mean_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4ff51",
   "metadata": {},
   "source": [
    "### What is the mean age of female persons? What about male persons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d4a83a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.92798913043478\n"
     ]
    }
   ],
   "source": [
    "female_mean_age = data_frame.loc[data_frame[\"sex\"] == \"Female\", \"age\"].mean()\n",
    "\n",
    "print(female_mean_age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33951ed3",
   "metadata": {},
   "source": [
    "### How many different types of educations are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e49ae",
   "metadata": {},
   "source": [
    "There are 16, i did a check with min max, to check starting counting value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "efff5cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "min = data_frame[\"education-num\"].min()\n",
    "max = data_frame[\"education-num\"].max()\n",
    "\n",
    "print(min)\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04662aba",
   "metadata": {},
   "source": [
    "### What are the different types of education and how many persons are the for each type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c94b97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th', 'Assoc-acdm', 'Assoc-voc', 'Bachelors', 'Doctorate', 'HS-grad', 'Masters', 'Preschool', 'Prof-school', 'Some-college']\n"
     ]
    }
   ],
   "source": [
    "education_types_overview = sorted(data_frame[\"education\"].unique())\n",
    "\n",
    "print(education_types_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9b699916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education\n",
      "HS-grad         15784\n",
      "Some-college    10878\n",
      "Bachelors        8025\n",
      "Masters          2657\n",
      "Assoc-voc        2061\n",
      "11th             1812\n",
      "Assoc-acdm       1601\n",
      "10th             1389\n",
      "7th-8th           955\n",
      "Prof-school       834\n",
      "9th               756\n",
      "12th              657\n",
      "Doctorate         594\n",
      "5th-6th           509\n",
      "1st-4th           247\n",
      "Preschool          83\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "people_pr_type = data_frame[\"education\"].value_counts()\n",
    "\n",
    "print(people_pr_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43131de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
